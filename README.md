# Отчёт

Изначальная выборка разбита на тренировочный (75%) и тестовый (25%) датасаеты. Входные данные отнормировал с помощью класса Normalise. В качестве метрики результата - MSE на выходе (P1, P2). Испытано две модели:
1) **SimpleNet**. Очень простая (один скрытый слой с 40 нейронами, ReLU фукция активации, активацию на выходе я забыл :) ). Достигнута точность порядка $L_{mse} \approx 0.001 \div 0.01$ 
2) **ReLuNet**. Чуть по-больше (три скрытых слоя: с 80, 120 и 80 нейронами, везде функция активации ReLU). С помощью ансамблирования 5 моделей достигнута точность $L_{mse} \approx 0.0001$ 

Модели сохранены в виде тензоров параметров. В ноутбуке представлена ворзможность их загрузить.

Процесс обучения модели **ReLuNet_01**:
![plot1](models/ReLuNet_01_train_loss.png)

Визуализация результата работы модели **ReLuNet_01**:
![plot2](models/ReLuNet_01_visualisation.png)

Ансамблирование 5 моделей **ReLuNet**. Сверху их собственные лоссы, снизу при усреднении выхода:
![plot3](models/ansamble.png)


# Requirements:
- pytroch, sklearn
- numpy, pandas
- seaborn, ipympl 

